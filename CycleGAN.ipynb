{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:38:41.809074Z",
     "start_time": "2020-12-29T10:38:41.213492Z"
    },
    "collapsed": true,
    "id": "14h4ejQKFVe9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:38:44.233272Z",
     "start_time": "2020-12-29T10:38:42.185558Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnlKaodtFVe9",
    "outputId": "dae2010b-a606-4c77-c3c8-7ccb1dd7d322"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print (tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lkpw_LFJf9fb",
    "outputId": "fd77c412-9e34-495b-a40c-b68c61b7ade0"
   },
   "outputs": [],
   "source": [
    "!wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5O2S_e6f9lZ",
    "outputId": "b8423974-4b0c-4a30-e1ee-f8e75e6eae09"
   },
   "outputs": [],
   "source": [
    "!unzip horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDReS1IBf9pK",
    "outputId": "a0fba3c3-390b-4fd2-ccbf-99e6cd446df9"
   },
   "outputs": [],
   "source": [
    "!ls horse2zebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:39:21.355638Z",
     "start_time": "2020-12-29T10:39:21.321029Z"
    },
    "collapsed": true,
    "id": "NQQBPNG2h-Cy"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"\"\n",
    "horses_train = glob.glob(path + 'horse2zebra/trainA/*.jpg')\n",
    "zebras_train = glob.glob(path + 'horse2zebra/trainB/*.jpg')\n",
    "horses_test = glob.glob(path + 'horse2zebra/testA/*.jpg')\n",
    "zebras_test = glob.glob(path + 'horse2zebra/testB/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:39:25.083696Z",
     "start_time": "2020-12-29T10:39:25.074044Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZS3Lq6rHh-Fe",
    "outputId": "f9578ca3-764b-4660-8236-9ffe72690a1d"
   },
   "outputs": [],
   "source": [
    "len(horses_train), len(zebras_train), len(horses_test), len(zebras_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:39:25.818535Z",
     "start_time": "2020-12-29T10:39:25.704968Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ0DxtPLh-hQ",
    "outputId": "8edca78e-31af-4a20-9999-cbb08f4d9379"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "for file in horses_train[:10]:\n",
    "    img = cv2.imread(file)\n",
    "    print (img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:01.168448Z",
     "start_time": "2020-12-29T10:39:59.517131Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "06Fm26_-h-kE",
    "outputId": "ce6ce15c-7657-4edd-aea4-5cf5cf2d3a46"
   },
   "outputs": [],
   "source": [
    "print (\"Horses\")\n",
    "for k in range(2):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for j in range(6):\n",
    "        file = np.random.choice(horses_train)\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(660 + 1 + j)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "print (\"-\"*80)\n",
    "print (\"Zebras\")\n",
    "for k in range(2):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for j in range(6):\n",
    "        file = np.random.choice(zebras_train)\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(660 + 1 + j)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxFTsuaGFVe-"
   },
   "source": [
    "# Generator Model (Res-Net Like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NUN7drfp66aD"
   },
   "outputs": [],
   "source": [
    "#Taken from: https://keras.io/examples/generative/cyclegan/\n",
    "\n",
    "class ReflectionPadding2D(tensorflow.keras.layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tensorflow.pad(input_tensor, padding_tensor, mode=\"REFLECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:03.471009Z",
     "start_time": "2020-12-29T10:40:03.416396Z"
    },
    "collapsed": true,
    "id": "TKq0iUEtc_JQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nEqBpfUr7aIU"
   },
   "outputs": [],
   "source": [
    "# Weights initializer for the layers.\n",
    "kernel_init = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_u-MUZNnAayH"
   },
   "outputs": [],
   "source": [
    "# c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n",
    "# d128 ====|\n",
    "#          |-> 2 downsampling blocks\n",
    "# d256 ====|\n",
    "# R256 ====|\n",
    "# R256     |\n",
    "# R256     |\n",
    "# R256     |\n",
    "# R256     |-> 9 residual blocks\n",
    "# R256     |\n",
    "# R256     |\n",
    "# R256     |\n",
    "# R256 ====|\n",
    "# u128 ====|\n",
    "#          |-> 2 upsampling blocks\n",
    "# u64  ====|\n",
    "# c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nS30wcwoSPta"
   },
   "outputs": [],
   "source": [
    "def custom_resnet_block(input_data, filters):\n",
    "    x = ReflectionPadding2D()(input_data)\n",
    "    x = tensorflow.keras.layers.Conv2D(filters, kernel_size=(3,3), padding='valid', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = tensorflow.keras.layers.Conv2D(filters, kernel_size=(3,3), padding='valid', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "\n",
    "    x = tensorflow.keras.layers.Add()([x, input_data])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-uLPpRpTkEr",
    "outputId": "af2a0318-a4e6-43e6-a6cb-9f92f53a6e20"
   },
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    source_image = tensorflow.keras.layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(source_image)\n",
    "    x = tensorflow.keras.layers.Conv2D(64, kernel_size=(7,7), kernel_initializer=kernel_init, use_bias=False)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tensorflow.keras.layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), padding='same', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tensorflow.keras.layers.Conv2D(256, kernel_size=(3,3), strides=(2,2), padding='same', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "    x = custom_resnet_block(x, 256)\n",
    "\n",
    "    x = tensorflow.keras.layers.Conv2DTranspose(128, kernel_size=(3,3), strides=(2,2), padding='same', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = tensorflow.keras.layers.Conv2DTranspose(64, kernel_size=(3,3), strides=(2,2), padding='same', kernel_initializer=kernel_init)(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "    x = tensorflow.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = tensorflow.keras.layers.Conv2D(3, kernel_size=(7,7), padding='valid')(x)\n",
    "    x = tfa.layers.InstanceNormalization()(x)\n",
    "\n",
    "    translated_image = tensorflow.keras.layers.Activation('tanh')(x)\n",
    "\n",
    "    return source_image, translated_image\n",
    "\n",
    "source_image, translated_image = make_generator()\n",
    "generator_network_AB = tensorflow.keras.models.Model(inputs=source_image, outputs=translated_image)\n",
    "\n",
    "source_image, translated_image = make_generator()\n",
    "generator_network_BA = tensorflow.keras.models.Model(inputs=source_image, outputs=translated_image)\n",
    "\n",
    "print (generator_network_AB.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQOF8YEKfoFm"
   },
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:07.021369Z",
     "start_time": "2020-12-29T10:40:07.016565Z"
    },
    "collapsed": true,
    "id": "gXTFSDKafoFm"
   },
   "outputs": [],
   "source": [
    "def my_conv_layer(input_layer, filters, strides, bn=True):\n",
    "    x = tensorflow.keras.layers.Conv2D(filters, kernel_size=(4,4), strides=strides, padding='same', kernel_initializer=kernel_init)(input_layer)\n",
    "    x = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    if bn:\n",
    "        #x = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(x)\n",
    "        x = tfa.layers.InstanceNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:08.703659Z",
     "start_time": "2020-12-29T10:40:08.398476Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GCaEdctfoFn",
    "outputId": "8c0f5a46-1698-46ea-961e-47e6cd0a745f"
   },
   "outputs": [],
   "source": [
    " def make_discriminator():\n",
    "    target_image_input = tensorflow.keras.layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "    x = my_conv_layer(target_image_input, 64, (2,2), bn=False)\n",
    "    x = my_conv_layer(x, 128, (2,2))\n",
    "    x = my_conv_layer(x, 256, (2,2))\n",
    "    x = my_conv_layer(x, 512, (1,1))\n",
    "\n",
    "    patch_features = tensorflow.keras.layers.Conv2D(1, kernel_size=(4,4), padding='same')(x)\n",
    "    return target_image_input, patch_features\n",
    "\n",
    "\n",
    "target_image_input, patch_features = make_discriminator()\n",
    "discriminator_network_A = tensorflow.keras.models.Model(inputs=target_image_input, outputs=patch_features)\n",
    "\n",
    "target_image_input, patch_features = make_discriminator()\n",
    "discriminator_network_B = tensorflow.keras.models.Model(inputs=target_image_input, outputs=patch_features)\n",
    "\n",
    "print (discriminator_network_A.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:08.726671Z",
     "start_time": "2020-12-29T10:40:08.704748Z"
    },
    "collapsed": true,
    "id": "tMc1CyNNFVe-"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_network_A.compile(loss='mse', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "discriminator_network_B.compile(loss='mse', optimizer=adam_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99IlAN3AFVe-"
   },
   "source": [
    "# Cycle-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:16.154143Z",
     "start_time": "2020-12-29T10:40:14.368064Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjnxFr21FVe-",
    "outputId": "b917bc9c-965e-4c45-ffac-ed681017c309"
   },
   "outputs": [],
   "source": [
    "source_image_A = tensorflow.keras.layers.Input(shape=(256, 256, 3))\n",
    "source_image_B = tensorflow.keras.layers.Input(shape=(256, 256, 3))\n",
    "\n",
    "# Domain Transfer\n",
    "fake_B = generator_network_AB(source_image_A)\n",
    "fake_A = generator_network_BA(source_image_B)\n",
    "\n",
    "# Restoring original Domain\n",
    "get_back_A = generator_network_BA(fake_B)\n",
    "get_back_B = generator_network_AB(fake_A)\n",
    "\n",
    "# Get back Identical/Same Image\n",
    "get_same_A = generator_network_BA(source_image_A)\n",
    "get_same_B = generator_network_AB(source_image_B)\n",
    "  \n",
    "discriminator_network_A.trainable=False\n",
    "discriminator_network_B.trainable=False\n",
    "\n",
    "# Tell Real vs Fake, for a given domain\n",
    "verify_A = discriminator_network_A(fake_A)\n",
    "verify_B = discriminator_network_B(fake_B)\n",
    "\n",
    "cycle_gan = tensorflow.keras.models.Model(inputs = [source_image_A, source_image_B], \\\n",
    "                              outputs = [verify_A, verify_B, get_back_A, get_back_B, get_same_A, get_same_B])\n",
    "cycle_gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFyjhkyCFVe_"
   },
   "source": [
    "# Compiling Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:39.847855Z",
     "start_time": "2020-12-29T10:40:39.834178Z"
    },
    "collapsed": true,
    "id": "rPG_olwXFVe_"
   },
   "outputs": [],
   "source": [
    "cycle_gan.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights=[1, 1, 10, 10, 5, 5],\\\n",
    "                                          optimizer=adam_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RpVWMO8FVe_"
   },
   "source": [
    "# Define Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:41.198876Z",
     "start_time": "2020-12-29T10:40:41.082243Z"
    },
    "collapsed": true,
    "id": "pTSivi-dFVe_"
   },
   "outputs": [],
   "source": [
    "def horses_to_zebras(horses, generator_network):\n",
    "    generated_samples = generator_network.predict_on_batch(horses)\n",
    "    return generated_samples\n",
    "\n",
    "def zebras_to_horses(zebras, generator_network):\n",
    "    generated_samples = generator_network.predict_on_batch(zebras)\n",
    "    return generated_samples\n",
    "\n",
    "def get_horse_samples(batch_size):\n",
    "    random_files = np.random.choice(horses_train, size=batch_size)\n",
    "    images = []\n",
    "    for file in random_files:\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append((img-127.5)/127.5)\n",
    "    horse_images = np.array(images)\n",
    "    return horse_images\n",
    "\n",
    "def get_zebra_samples(batch_size):\n",
    "    random_files = np.random.choice(zebras_train, size=batch_size)\n",
    "    images = []\n",
    "    for file in random_files:\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append((img-127.5)/127.5)\n",
    "    zebra_images = np.array(images)\n",
    "    return zebra_images\n",
    "\n",
    "def show_generator_results_horses_to_zebras(generator_network_AB, generator_network_BA):\n",
    "    images = []\n",
    "    for j in range(5):\n",
    "        file = np.random.choice(horses_test)\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "\n",
    "    print ('Input Horse Images')\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(images):\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "    print ('Translated (Horse -> Zebra) Images')\n",
    "    translated = []\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(images):\n",
    "        img = (img-127.5)/127.5\n",
    "        output = horses_to_zebras(np.array([img]), generator_network_AB)[0]\n",
    "        translated.append(output)\n",
    "        output = (output+1.0)/2.0\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "    print ('Translated reverse ( Fake Zebras -> Fake Horses)')\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(translated):\n",
    "        output = zebras_to_horses(np.array([img]), generator_network_BA)[0]\n",
    "        output = (output+1.0)/2.0\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "def show_generator_results_zebras_to_horses(generator_network_AB, generator_network_BA):\n",
    "    images = []\n",
    "    for j in range(5):\n",
    "        file = np.random.choice(zebras_test)\n",
    "        img = cv2.imread(file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "\n",
    "    print ('Input Zebra Images')\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(images):\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "    print ('Translated (Zebra -> Horse) Images')\n",
    "    translated = []\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(images):\n",
    "        img = (img-127.5)/127.5\n",
    "        output = zebras_to_horses(np.array([img]), generator_network_BA)[0]\n",
    "        translated.append(output)\n",
    "        output = (output+1.0)/2.0\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()\n",
    "\n",
    "    print ('Translated reverse (Fake Horse -> Fake Zebra)')\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    for j, img in enumerate(translated):\n",
    "        output = horses_to_zebras(np.array([img]), generator_network_AB)[0]\n",
    "        output = (output+1.0)/2.0\n",
    "        plt.subplot(550 + 1 + j)\n",
    "        plt.imshow(output)\n",
    "        plt.axis('off')\n",
    "        #plt.title(trainY[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEOpY_BZFVe_"
   },
   "source": [
    "# Training GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T10:40:47.163972Z",
     "start_time": "2020-12-29T10:40:47.160269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lg6OKd4qmIdh",
    "outputId": "91122f74-6d1a-41d2-a9b2-8eec9404a175"
   },
   "outputs": [],
   "source": [
    "len(horses_train), len(zebras_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-29T10:40:50.895Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "E_nihCaQFVe_",
    "outputId": "5124a23b-f386-47ab-9007-67f0c4f3130d"
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 1\n",
    "steps = 1067\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    if i%5 == 0:\n",
    "        show_generator_results_horses_to_zebras(generator_network_AB, generator_network_BA)\n",
    "        print (\"-\"*100)\n",
    "        show_generator_results_zebras_to_horses(generator_network_AB, generator_network_BA)\n",
    "    for j in range(steps):  \n",
    "        # A == Horses\n",
    "        # B == Zebras\n",
    "        domain_A_images = get_horse_samples(batch_size)\n",
    "        domain_B_images = get_zebra_samples(batch_size)\n",
    "\n",
    "        fake_patch = np.zeros((batch_size, 32, 32, 1))\n",
    "        real_patch = np.ones((batch_size, 32, 32, 1))\n",
    "        \n",
    "        fake_B_images = generator_network_AB(domain_A_images)\n",
    "        fake_A_images = generator_network_BA(domain_B_images)\n",
    "        \n",
    "        # Updating Discriminator A weights\n",
    "        discriminator_network_A.trainable=True\n",
    "        discriminator_network_B.trainable=False\n",
    "        loss_d_real_A = discriminator_network_A.train_on_batch(domain_A_images, real_patch)\n",
    "        loss_d_fake_A = discriminator_network_A.train_on_batch(fake_A_images, fake_patch)\n",
    "        \n",
    "        loss_d_A = np.add(loss_d_real_A, loss_d_fake_A)/2.0\n",
    "        \n",
    "        # Updating Discriminator B weights\n",
    "        discriminator_network_B.trainable=True\n",
    "        discriminator_network_A.trainable=False\n",
    "        loss_d_real_B = discriminator_network_B.train_on_batch(domain_B_images, real_patch)\n",
    "        loss_d_fake_B = discriminator_network_B.train_on_batch(fake_B_images, fake_patch)\n",
    "        \n",
    "        loss_d_B = np.add(loss_d_real_B, loss_d_fake_B)/2.0\n",
    "        \n",
    "        # Make the Discriminator belive that these are real samples and calculate loss to train the generator\n",
    "        \n",
    "        discriminator_network_A.trainable=False\n",
    "        discriminator_network_B.trainable=False\n",
    "        \n",
    "        # Updating Generator weights\n",
    "        loss_g = cycle_gan.train_on_batch([domain_A_images, domain_B_images],\\\n",
    "                    [real_patch, real_patch, domain_A_images, domain_B_images, domain_A_images, domain_B_images])\n",
    "        \n",
    "        if j%100 == 0:\n",
    "            print (\"Epoch:%.0f, Step:%.0f, DA-Loss:%.3f, DA-Acc:%.3f, DB-Loss:%.3f, DB-Acc:%.3f, G-Loss:%.3f\"\\\n",
    "                   %(i,j,loss_d_A[0],loss_d_A[1]*100,loss_d_B[0],loss_d_B[1]*100,loss_g[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hIKM9tkYGl26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cycle-GAN-Horses-Zebras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
